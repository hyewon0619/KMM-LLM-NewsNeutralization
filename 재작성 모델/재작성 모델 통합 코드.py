# ë°ì´í„°

## ë°ì´í„°ì…‹ ìƒì„±

import pandas as pd

truth_df = pd.read_csv("/content/drive/MyDrive/á„ƒá…¦á„‰á…¡ á„á…¢á†¸á„‰á…³á„á…©á†«/ë°ì´í„°/ì§„ìœ„ ë°ì´í„°.csv")

truth_sample = truth_df[truth_df['fake_label'] == 1].sample(1000, random_state=42)
truth_sample = truth_sample[['title']].rename(columns={'title': 'text'})

from datasets import load_dataset

sentiment_ds = load_dataset("tweet_eval", "sentiment")
# label 0(negative) ë˜ëŠ” 2(positive)ì¸ ìƒ˜í”Œ 1000ê°œ
sentiment_df = pd.DataFrame(sentiment_ds['train'])
sentiment_sample = sentiment_df[sentiment_df['label'].isin([0,2])].sample(1000, random_state=42)
sentiment_sample = sentiment_sample[['text']]

# ===== 3. í¸í–¥ ë°ì´í„° =====
bias_df = pd.read_csv("/content/drive/MyDrive/á„ƒá…¦á„‰á…¡ á„á…¢á†¸á„‰á…³á„á…©á†«/ë°ì´í„°/Qbias/allsides_balanced_news_headlines-texts.csv")
# bias_ratingì´ left ë˜ëŠ” rightì¸ ìƒ˜í”Œ 1000ê°œ, title+heading+text í•©ì¹˜ê¸°
bias_df_filtered = bias_df[bias_df['bias_rating'].isin(['left','right'])].sample(1000, random_state=42)
bias_sample = bias_df_filtered['title'] + " " + bias_df_filtered['heading'] + " " + bias_df_filtered['text']
bias_sample = pd.DataFrame({'text': bias_sample})

# ===== 4. 3ê°œ í†µí•© =====
combined_df = pd.concat([truth_sample, sentiment_sample, bias_sample], ignore_index=True)
print(combined_df.shape)
print(combined_df.head())

# ===== 5. í•„ìš”í•˜ë©´ CSVë¡œ ì €ì¥ =====
combined_df.to_csv("/content/drive/MyDrive/á„ƒá…¦á„‰á…¡ á„á…¢á†¸á„‰á…³á„á…©á†«/ë°ì´í„°/rewrite_training_data.csv", index=False)

## ì¤‘ë¦½ì  ë¬¸ì¥ ë°ì´í„°ì…‹

from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch

model_name = "t5-base"
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

texts = combined_df["text"].tolist()

def make_prompt(text):
    return f"Rewrite this news headline to be neutral: {text}"

from torch.utils.data import DataLoader

batch_size = 16  # GPU ë©”ëª¨ë¦¬ ìƒí™©ì— ë§ê²Œ ì¡°ì ˆ

outputs = []

for i in range(0, len(texts), batch_size):
    batch = texts[i:i+batch_size]
    prompts = [make_prompt(t) for t in batch]
    encodings = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True).to(device)
    generated_ids = model.generate(**encodings, max_length=256)
    decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
    outputs.extend(decoded)

combined_df["neutral_text"] = outputs
combined_df.to_csv("/content/drive/MyDrive/ë°ì‚¬ ìº¡ìŠ¤í†¤/rewrite_neutral_texts.csv", index=False)

print(combined_df)
print(combined_df['neutral_text'])

# 1ï¸âƒ£ í”„ë¡¬í”„íŠ¸ ì œê±°
combined_df["neutral_text"] = combined_df["neutral_text"].str.replace(
    r"(Rewrite this news headline to be neutral:|be neutral:)\s*", "", regex=True
).str.lstrip(": ").str.strip()

# 2ï¸âƒ£ ì‹œì‘ì´ ':'ë¡œ ì‹œì‘í•˜ë©´ ì œê±°
combined_df["neutral_text"] = combined_df["neutral_text"].str.lstrip(": ").str.strip()

# 3ï¸âƒ£ ë„ê°’ í™•ì¸
null_count = combined_df["neutral_text"].isnull().sum()
print(f"ë„ê°’ ê°œìˆ˜: {null_count}")

# 4ï¸âƒ£ ê²°ê³¼ í™•ì¸
print(combined_df.head())

empty_count = (combined_df["neutral_text"].isnull() | (combined_df["neutral_text"].str.strip() == "")).sum()
print(f"ë„ í˜¹ì€ ë¹ˆ ë¬¸ìì—´ ê°œìˆ˜: {empty_count}")

combined_df = combined_df[combined_df["neutral_text"].str.strip() != ""]
combined_df = combined_df.dropna(subset=["neutral_text"]).reset_index(drop=True)

print(f"ì‚­ì œ í›„ ë°ì´í„° ê°œìˆ˜: {combined_df.shape[0]}")

combined_df.to_csv("/content/drive/MyDrive/ë°ì‚¬ ìº¡ìŠ¤í†¤/rewrite_neutral_texts.csv", index=False)

import pandas as pd

# ì „ì²´ ì—´ ë‚´ìš© ë‹¤ ë³´ì´ê²Œ ì„¤ì •
pd.set_option('display.max_colwidth', None)

# 0~4í–‰ë§Œ ë³´ê¸°
print(combined_df.loc[0:4])

import pandas as pd
import re

# 1ï¸âƒ£ í”„ë¡¬í”„íŠ¸ ë¬¸êµ¬ ì œê±°
combined_df["neutral_text"] = combined_df["neutral_text"].str.replace(
    r"^(Rewrite this news headline to be neutral:|be neutral:)\s*", "", regex=True
)

# 2ï¸âƒ£ ì‹œì‘ì´ ':'ë‚˜ ê³µë°±ìœ¼ë¡œ ì‹œì‘í•˜ë©´ ì œê±°
combined_df["neutral_text"] = combined_df["neutral_text"].str.lstrip(": ").str.strip()

# 3ï¸âƒ£ ì¤‘ë³µ ë°˜ë³µ ì œê±° (ê°™ì€ ë¬¸ì¥ì´ ë°˜ë³µë˜ë©´ í•œ ë²ˆë§Œ)
def remove_repeats(text):
    if pd.isnull(text) or text.strip() == "":
        return ""
    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    sentences = re.split(r'(?<=[.!?]) +', text)
    seen = set()
    cleaned = []
    for s in sentences:
        s_clean = s.strip()
        if s_clean and s_clean not in seen:
            cleaned.append(s_clean)
            seen.add(s_clean)
    return " ".join(cleaned)

combined_df["neutral_text"] = combined_df["neutral_text"].apply(remove_repeats)

# 4ï¸âƒ£ ë„ ë˜ëŠ” ë¹ˆ ë¬¸ìì—´ í–‰ ì œê±°
combined_df = combined_df[combined_df["neutral_text"].str.strip() != ""]

# 5ï¸âƒ£ ê²°ê³¼ í™•ì¸
print("ì •ë¦¬ í›„ ë°ì´í„° ìˆ˜:", combined_df.shape[0])
print(combined_df.head())

## Wiki Neutrality Corpus (WNC) ë°ì´í„°

import pandas as pd

wnc_df = pd.read_csv(
    '/content/drive/MyDrive/ë°ì‚¬ ìº¡ìŠ¤í†¤/ë°ì´í„°/biased.full',
    sep='\t',
    names=[
        "id", "src_tok", "tgt_tok", "src_raw", "tgt_raw", "src_POS_tags", "tgt_parse_tags"
    ],
    quoting=3,             # ë”°ì˜´í‘œ ë¬¸ì œ ë°©ì§€
    on_bad_lines='skip',   # ë¬¸ì œ ìˆëŠ” ì¤„ ê±´ë„ˆë›°ê¸°
    engine='python'        # íŒŒì„œ ì•ˆì •ì„± ë†’ì´ê¸°
)

# í¸í–¥ ë¬¸ì¥ â†” ì¤‘ë¦½ ë¬¸ì¥ ì„ íƒ
train_df = wnc_df[["src_raw", "tgt_raw"]].rename(
    columns={"src_raw": "biased_text", "tgt_raw": "neutral_text"}
)

# ê²°ì¸¡ì¹˜ ì œê±°
train_df = train_df.dropna(subset=["biased_text", "neutral_text"]).reset_index(drop=True)

# ë¯¸ë¦¬ë³´ê¸°
print(train_df.sample(5))

train_df

# ì €ì¥
train_df.to_csv('/content/drive/MyDrive/ë°ì‚¬ ìº¡ìŠ¤í†¤/ë°ì´í„°/ì¬ì‘ì„± ë°ì´í„°.csv', index=False)

# ì¬ì‘ì„± ëª¨ë¸ í›ˆë ¨

import pandas as pd

df = pd.read_csv("/content/drive/MyDrive/á„ƒá…¦á„‰á…¡ á„á…¢á†¸á„‰á…³á„á…©á†«/á„ƒá…¦á„‹á…µá„á…¥/á„Œá…¢á„Œá…¡á†¨á„‰á…¥á†¼ á„ƒá…¦á„‹á…µá„á…¥.csv")
print(df)

## T5-base ëª¨ë¸

!pip install transformers datasets accelerate

!pip install transformers datasets accelerate evaluate

!pip install rouge_score

!pip install --upgrade transformers datasets evaluate rouge_score

from datasets import Dataset
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
import math

# í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì…ë ¥/ì¶œë ¥)
df["input_text"] = "Neutralize the following sentence: " + df["biased_text"]
df["target_text"] = df["neutral_text"]

# Hugging Face Dataset ë³€í™˜
dataset = Dataset.from_pandas(df[["input_text", "target_text"]])

# í† í¬ë‚˜ì´ì € ë¡œë“œ
from transformers import T5Tokenizer
tokenizer = T5Tokenizer.from_pretrained("t5-base")

def preprocess(batch):
    inputs = tokenizer(batch["input_text"], padding="max_length", truncation=True, max_length=256)
    labels = tokenizer(batch["target_text"], padding="max_length", truncation=True, max_length=256)
    inputs["labels"] = [
        [(label if label != tokenizer.pad_token_id else -100) for label in label_seq]
        for label_seq in labels["input_ids"]
    ]
    return inputs

# ëª¨ë¸ ë¡œë“œ
import evaluate
from transformers import T5ForConditionalGeneration
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments
model = T5ForConditionalGeneration.from_pretrained("t5-base")
import numpy as np

# ====== ğŸ“Š í‰ê°€ í•¨ìˆ˜ (ROUGE) ======
rouge = evaluate.load("rouge")

def compute_metrics(eval_pred):
    preds, labels = eval_pred

    # predsê°€ tupleë¡œ ë“¤ì–´ì˜¬ ê²½ìš° flatten
    if isinstance(preds, tuple):
        preds = preds[0]

    # logitsì¼ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ argmax ì²˜ë¦¬
    if hasattr(preds, "ndim") and preds.ndim > 1:
        preds = np.argmax(preds, axis=-1)

    # âœ… labelsì— -100ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ tokenizerê°€ ë””ì½”ë”© ëª» í•¨
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)

    # predsì™€ labelsê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ ê²½ìš° ëŒ€ë¹„ flatten
    preds = preds.tolist() if isinstance(preds, np.ndarray) else preds
    labels = labels.tolist() if isinstance(labels, np.ndarray) else labels

    # ë””ì½”ë”©
    decoded_preds = [tokenizer.decode(p, skip_special_tokens=True) for p in preds]
    decoded_labels = [tokenizer.decode(l, skip_special_tokens=True) for l in labels]

    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)
    return {k: round(v * 100, 2) for k, v in result.items()}

i_start = 0  # ì˜ˆ: ì´ë¯¸ 2ë°°ì¹˜ í•™ìŠµ ì™„ë£Œ â†’ i_start = 2

batch_size = 1000
num_batches = (len(dataset) + batch_size - 1) // batch_size  # ì „ì²´ ë°°ì¹˜ë¥¼ ê³„ì‚°

for i in range(i_start, num_batches):
    print(f"===== í•™ìŠµ ë°°ì¹˜ {i+1}/{num_batches} =====")

    start = i * batch_size
    end = start + batch_size
    subset = dataset.select(range(start, min(end, len(dataset))))
    subset = subset.train_test_split(test_size=0.05, seed=42)
    tokenized_subset = subset.map(preprocess, batched=True)

    # ì´ì „ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    if i == 0 and i_start == 0:
        model_path = "t5-base"  # ì²˜ìŒì´ë©´ t5-base ì´ˆê¸°í™”
    else:
        model_path = f"/content/t5-neutralizer-batch{i}"  # ì´ì „ ë°°ì¹˜ ëª¨ë¸

    model = T5ForConditionalGeneration.from_pretrained(model_path)

    training_args = Seq2SeqTrainingArguments(
        output_dir= f"/content/t5-neutralizer-batch{i+1}",
        per_device_train_batch_size=4,  # GPU ìƒí™©ì— ë§ì¶° ì¡°ì ˆ
        per_device_eval_batch_size=4,
        num_train_epochs=1,
        logging_steps=50,
        save_total_limit=1,
        eval_strategy="epoch", # no
        save_strategy="epoch",
        learning_rate=5e-5,
        fp16=True,
        load_best_model_at_end=True, #False
        predict_with_generate=True,
        report_to="none",
    )

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_subset["train"],
        eval_dataset=tokenized_subset["test"],
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )

    trainer.train()

    # í•™ìŠµ í›„ ì €ì¥
    trainer.save_model(f"/content/t5-neutralizer-batch{i+1}")
    tokenizer.save_pretrained(f"/content/t5-neutralizer-batch{i+1}")

    # if i > 2:  # ì €ì¥ ì‹œí‚¨ ëª¨ë¸ì˜ ì „ì „ ëª¨ë¸ ì‚­ì œ
    #   shutil.rmtree(f"/content/t5-neutralizer-batch{i-1}")

    print(f"===== ë°°ì¹˜ {i+1} í•™ìŠµ ì™„ë£Œ ë° ì €ì¥ =====\n")

# í•™ìŠµ ë²„ì „ 2

import evaluate
# ====== ğŸ“Š í‰ê°€ í•¨ìˆ˜ (ROUGE) ======
rouge = evaluate.load("rouge")
def compute_metrics(eval_pred):
    preds, labels = eval_pred
    if isinstance(preds, tuple):
        preds = preds[0]
    if hasattr(preds, "ndim") and preds.ndim > 1:
        preds = np.argmax(preds, axis=-1)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    preds = preds.tolist() if isinstance(preds, np.ndarray) else preds
    labels = labels.tolist() if isinstance(labels, np.ndarray) else labels
    decoded_preds = [tokenizer.decode(p, skip_special_tokens=True) for p in preds]
    decoded_labels = [tokenizer.decode(l, skip_special_tokens=True) for l in labels]
    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)
    return {k: round(v * 100, 2) for k, v in result.items()}

## 2000ê°œì”© ëˆ„ì  í•™ìŠµ

import torch  # ğŸ”¹ ì¶”ê°€
import evaluate
from transformers import T5ForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments
import numpy as np
import os

# ëª¨ë¸ ì´ˆê¸°í™”
model = T5ForConditionalGeneration.from_pretrained("t5-base")


i_start = 0   # ì´ê±° ê°±ì‹ í•˜ê¸° !!!!!!!!!!!!!!

batch_size = 2000
num_batches = (len(dataset) + batch_size - 1) // batch_size

for i in range(i_start, 8):
    print(f"===== í•™ìŠµ ë°°ì¹˜ {i+1}/{num_batches} =====")

    start = i * batch_size
    end = start + batch_size
    subset = dataset.select(range(start, min(end, len(dataset))))
    subset = subset.train_test_split(test_size=0.05, seed=42)
    tokenized_subset = subset.map(preprocess, batched=True)

    # ğŸ”¹ ì´ì „ ë°°ì¹˜ í•™ìŠµ weight ë¶ˆëŸ¬ì˜¤ê¸° (state_dict ë°©ì‹)
    if i > 0:
        state_path = f"/content/drive/MyDrive/á„ƒá…¦á„‰á…¡ á„á…¢á†¸á„‰á…³á„á…©á†«/á„†á…©á„ƒá…¦á†¯/t5-neutralizer-batch{i}_weights.pt"
        model.load_state_dict(torch.load(state_path))
        print(f"ğŸ”¹ ì´ì „ ë°°ì¹˜ weight({state_path}) ë¶ˆëŸ¬ì˜´")

    training_args = Seq2SeqTrainingArguments(
        output_dir=f"/content/drive/MyDrive/á„ƒá…¦á„‰á…¡ á„á…¢á†¸á„‰á…³á„á…©á†«/á„†á…©á„ƒá…¦á†¯/t5-neutralizer-batch{i+1}",
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        num_train_epochs=1,
        logging_steps=50,
        save_total_limit=1,
        eval_strategy="epoch",
        save_strategy="epoch",
        learning_rate=5e-5,
        fp16=True,
        load_best_model_at_end=True,
        predict_with_generate=True,
        report_to="none",
    )

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_subset["train"],
        eval_dataset=tokenized_subset["test"],
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )

    trainer.train()

    # ğŸ”¹ í•™ìŠµ í›„ state_dict ì €ì¥
    state_path = f"/content/drive/MyDrive/á„ƒá…¦á„‰á…¡ á„á…¢á†¸á„‰á…³á„á…©á†«/á„†á…©á„ƒá…¦á†¯/t5-neutralizer-batch{i+1}_weights.pt"
    torch.save(model.state_dict(), state_path)
    print(f"ğŸ”¹ ë°°ì¹˜ {i+1} weight ì €ì¥: {state_path}")

    # ğŸ”¹ tokenizer ì €ì¥ (ë³€í™” ì—†ìŒ)
    tokenizer.save_pretrained(f"/content/drive/MyDrive/á„ƒá…¦á„‰á…¡ á„á…¢á†¸á„‰á…³á„á…©á†«/á„†á…©á„ƒá…¦á†¯/t5-neutralizer-batch{i+1}")

    print(f"===== ë°°ì¹˜ {i+1} í•™ìŠµ ì™„ë£Œ ë° ì €ì¥ =====\n")

# ì‚­ì œ

!rm -rf /content/t5-neutralizer-batch2*

## ìµœì  ì„ íƒ -> 5000ê°œ 1ì°¨

import torch
from transformers import T5ForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments
import numpy as np

# 1ï¸âƒ£ ëª¨ë¸ ì´ˆê¸°í™”
model = T5ForConditionalGeneration.from_pretrained("t5-base")
model.load_state_dict(torch.load("/content/drive/MyDrive/á„ƒá…¦á„‰á…¡ á„á…¢á†¸á„‰á…³á„á…©á†«/á„†á…©á„ƒá…¦á†¯/t5-neutralizer-batch4_weights.pt"))

# 2ï¸âƒ£ 5000ê°œ ë°ì´í„° ì„ íƒ ë° tokenization
subset = dataset.select(range(5000))
subset = subset.train_test_split(test_size=0.05, seed=42)
tokenized_subset = subset.map(preprocess, batched=True)

# 3ï¸âƒ£ Trainer ì„¤ì •
training_args = Seq2SeqTrainingArguments(
    output_dir="/content/drive/MyDrive/á„ƒá…¦á„‰á…¡ á„á…¢á†¸á„‰á…³á„á…©á†«/á„†á…©á„ƒá…¦á†¯/t5-neutralizer-final1",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    logging_steps=50,
    save_strategy="epoch",
    eval_strategy="epoch",
    learning_rate=5e-5,
    fp16=True,
    load_best_model_at_end=True,
    predict_with_generate=True,
    report_to="none",
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_subset["train"],
    eval_dataset=tokenized_subset["test"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# 4ï¸âƒ£ í•™ìŠµ ì‹œì‘
#trainer.train()
# ì´ì–´ì„œ
trainer.train(resume_from_checkpoint=False)

# 5ï¸âƒ£ í•™ìŠµ í›„ weight ì €ì¥
torch.save(model.state_dict(), "/content/drive/MyDrive/á„ƒá…¦á„‰á…¡ á„á…¢á†¸á„‰á…³á„á…©á†«/á„†á…©á„ƒá…¦á†¯/t5-neutralizer-final1_weights.pt")
tokenizer.save_pretrained("/content/drive/MyDrive/á„ƒá…¦á„‰á…¡ á„á…¢á†¸á„‰á…³á„á…©á†«/á„†á…©á„ƒá…¦á†¯/t5-neutralizer-final1")

## 10000ê°œ 2ì°¨ (íŒŒë¼ë¯¸í„° ì¡°ì •)

import torch
from transformers import T5ForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments
import numpy as np

# ëª¨ë¸ weight ë¶ˆëŸ¬ì˜¤ê¸°
model = T5ForConditionalGeneration.from_pretrained("t5-base")
model.load_state_dict(torch.load("/content/drive/MyDrive/á„ƒá…¦á„‰á…¡ á„á…¢á†¸á„‰á…³á„á…©á†«/á„†á…©á„ƒá…¦á†¯/t5-neutralizer-final1_weights.pt"))

# ë°ì´í„° ì¤€ë¹„ (10000ê°œ)
subset = dataset.select(range(10000))
subset = subset.train_test_split(test_size=0.05, seed=None)  # ì‹œë“œ ì—†ì• ì„œ ë‹¤ì–‘í™”
tokenized_subset = subset.map(preprocess, batched=True)

# Trainer ì„¤ì •
training_args = Seq2SeqTrainingArguments(
    output_dir="/content/drive/MyDrive/á„ƒá…¦á„‰á…¡ á„á…¢á†¸á„‰á…³á„á…©á†«/á„†á…©á„ƒá…¦á†¯/t5-neutralizer-final2",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=2,
    logging_steps=50,
    save_strategy="epoch",
    eval_strategy="epoch",
    learning_rate=5e-5,
    fp16=True,
    load_best_model_at_end=True,  # ì²´í¬í¬ì¸íŠ¸
    predict_with_generate=True,
    report_to="none",
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_subset["train"],
    eval_dataset=tokenized_subset["test"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# í•™ìŠµ ì‹œì‘
trainer.train()

# ìµœì¢… weight ì €ì¥
torch.save(model.state_dict(), "/content/drive/MyDrive/á„ƒá…¦á„‰á…¡ á„á…¢á†¸á„‰á…³á„á…©á†«/á„†á…©á„ƒá…¦á†¯/t5-neutralizer-final2_weights.pt")
tokenizer.save_pretrained("/content/drive/MyDrive/á„ƒá…¦á„‰á…¡ á„á…¢á†¸á„‰á…³á„á…©á†«/á„†á…©á„ƒá…¦á†¯/t5-neutralizer-final2")
